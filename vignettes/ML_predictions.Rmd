---
title: "ML predictions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ML predictions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.height = 9,
  fig.width = 9, 
  comment = "#>"
)
```


# Location Prediction

Machine learning predicts the location of the country of study of each paper.
The training labels were provided by human-reading randomly chosen articles from the corpus (1,428 human-derived labels) and from text mining the article metadata (2,663 text-mined labels).
Interestingly, the human-reading provided 563 observations of irrelevant country locations or irrelevant subjects of study.

The human-derived labels are first used for constructing a relevance filter based on simple binary classification between "Relevant" and "Irrelevant" documents ($n = 1,386$).
The predictors consist of the text document term matrix derived from the cleaned corpus text using tokens related to country names and of the topic membership output by the topic model ($p = 138$).
A benchmark of the six following models was conducted: featureless (baseline), random forest, support vector machine, naive Bayes, multinomial regression and extreme gradient boosting.
The hyper-parameters are initially set at standard default value.
The resampling scheme is 10 repeats of 10-fold cross-validation.
The random forest, multinomal and support vector machine models are the best-performing model and show no statistical difference in the distribution of their performance measured by area-under-curve (AUC).
These models are selected for further tuning using a nested cross-validation with a simple hold-out inner loop and 10 repeats of 10-fold cross-validation as outer loop.
The size of the tuning grid is set to 16 between standard values for each hyper-parameters.
Random forest and multinomial are the best-performing tuned models and show no statistical difference in the distribution of their performance measured by AUC.
The distribution of the hyper-parameter resulting from the nested cross-validation shows a better constraint for the multinomial model and it is selected for predictions.
The performance of such multinomial model corresponds to a mean AUC of 0.82, a mean accuracy of 77% and a true positive rate of 86%.

The prediction of the location of study for each document is performed using both human-derived and text-mined labels ($n = 3,494$).
The predictors consist of the text document term matrix derived from the cleaned corpus text using tokens related to country names ($p = 33$).
Models and resampling scheme are similar than the ones used for the relevance filter.
The hyper-parameters are initially set at standard default value.
Random forest outperforms every other model with a mean multiclass AUC of 0.99 and a mean accuracy of 96%.
No further tuning is performed and the random forest with default hyper-parameters is used for predictions.
More complex approaches were investigated: using additional geographical tokens (e.g. rivers and mountain ranges names), multilabel classifications (e.g. binary relevance, label powerset) or deep learning models of natural language processing (e.g. BERT) both on full texts and abstracts with little benefits.

# Libraries

```{r}
library(wateReview)
```


# Defining parameters

```{r}
SCALE_TYPE <- "location"
MODEL_TYPE <- "multiclass" # multiclass or binary_relevance
AGGREGATE <- FALSE
```


# Data reading

```{r}
topicDocs <- get_topicDocs()
titleDocs <- get_titleDocs(topicDocs)
validationHumanReading <-  get_validationHumanReading(scale_type = SCALE_TYPE)
DTM <- get_DocTermMatrix()
webscrapped_validationDTM <- get_webscrapped_validationDTM()
colnames(webscrapped_validationDTM) <- colnames(DTM)
webscrapped_validationDTM <- transform_DTM(webscrapped_validationDTM)
DTM <- transform_DTM(DTM)
webscrapped_trainingLabels <- get_webscrapped_trainingLabels()
titleInd <- get_titleInd(humanReadingDatabase = validationHumanReading, topicModelTitles = titleDocs)
```

# Sanity check

We now check if all the papers are found, address issues and align databases.

```{r}
table(is.na(titleInd))
alignedData <- align_humanReadingTopicModel(titleInd, validationHumanReading, topicDocs, DTM)
alignedData <- QA_alignedData(alignedData, scale_type = SCALE_TYPE)
validationTopicDocs <- alignedData$validationTopicDocs
validationHumanReading <- alignedData$validationHumanReading
validationHumanReadingDTM <- alignedData$validationDTM
```

# Get the training data from `humanReadingTrainingLabels`

```{r}
humanReadingTrainingLabels <- make_humanReadingTrainingLabels(validationHumanReading, scale_type = SCALE_TYPE, webscrapped_trainingLabels)
trainingData <- make_trainingData(validationHumanReadingDTM, humanReadingTrainingLabels, webscrapped_validationDTM, webscrapped_trainingLabels, scale_type = SCALE_TYPE, aggregate_labels = AGGREGATE)
head(trainingData, 5) %>%
	knitr::kable(digits = 3, format = "html", caption = "") %>% 
	kableExtra::kable_styling(bootstrap_options = c("hover", "condensed")) %>% 
	kableExtra::scroll_box(width = "7in")
```

# Exploratory data analysis

```{r}
EDA_trainingData(trainingData, validationHumanReadingDTM, humanReadingTrainingLabels) %>%
	knitr::kable(digits = 3, format = "html", caption = "") %>% 
	kableExtra::kable_styling(bootstrap_options = c("hover", "condensed")) %>% 
	kableExtra::scroll_box(width = "7in", height = "7in")
```

# Multilabel: binary relevance and algorithm adaptation

The following code executes a multilabl benchmark.

```
bmr <- multilabelBenchmark(trainingData, validationHumanReadingDTM, MODEL_TYPE, scale_type = SCALE_TYPE, aggregated_labels = AGGREGATE, obs_threshold = 10)
AggrPerformances <- getBMRAggrPerformances(bmr, as.df = TRUE)
PerfVisMultilabel(AggrPerformances)
```

# Multiclass

## Relevance filter

```
trainingDataMulticlassFilter <- make_trainingDataMulticlass(trainingData, validationHumanReadingDTM, humanReadingTrainingLabels, webscrapped_validationDTM, webscrapped_trainingLabels, 
	filter = TRUE, 
	addTopicDocs = TRUE, 
	validationTopicDocs = validationTopicDocs)
```

### Selecting a model to tune

```
if (!file.exists("bmr_filter.Rds")){
	bmr_filter <- multiclassBenchmark(trainingDataMulticlassFilter, MODEL_TYPE, filter = TRUE)
	saveRDS(bmr_filter, "bmr_filter.Rds")
} else {
	bmr_filter <- readRDS("bmr_filter.Rds")
}
make_AUCPlot(bmr_filter, binary = TRUE)
```

Based on the result of the AUC plot comparison, svm, multinom and RF are selected for tuning benchmark

### Tuning model

```
if(!file.exists("bmr_tune_filter.Rds")){
	bmr_tune_filter <- multiclassBenchmark(trainingDataMulticlassFilter, MODEL_TYPE, filter = TRUE, tune = list("classif.svm", "classif.randomForest", "classif.multinom"))
	saveRDS(bmr_tune_filter, "bmr_tune_filter.Rds")
} else {
	bmr_tune_filter <- readRDS("bmr_tune_filter.Rds")
}
make_AUCPlot(bmr_tune_filter, binary = TRUE)
```

RF and multninomal have similar performance, we pick multinom (better hyper-parameter constrains).

### Predict

```
targetData <- make_targetData(DTM, addTopicDocs = TRUE, topicDocs = topicDocs)
predRelevance <- make_predictions("classif.multinom", 
	list(mtry = get_tuningPar(bmr_tune_filter, "decay")),
	trainingDataMulticlassFilter, targetData, MODEL_TYPE, filter = TRUE)
table(predRelevance)
```

## Country

```
trainingDataMulticlass <- make_trainingDataMulticlass(trainingData, validationHumanReadingDTM, humanReadingTrainingLabels, webscrapped_validationDTM, webscrapped_trainingLabels, filter = FALSE, addWebscrapped = TRUE)
```

### Selecting a model to tune

```
if (!file.exists("bmr_country.Rds")){
	bmr_country <- multiclassBenchmark(trainingDataMulticlass, MODEL_TYPE, filter = FALSE)
	saveRDS(bmr_country, "bmr_country.Rds")
} else {
	bmr_country <- readRDS("bmr_country.Rds")
}
make_AUCPlot(bmr_country)
```

### Tuning

```
# bmr_tune_country <- multiclassBenchmark(trainingDataMulticlass, MODEL_TYPE, filter = FALSE, tune = list("classif.randomForest"))
# make_AUCPlot(bmr_tune_country)
# saveRDS(bmr_tune_country, "bmr_tune_country.Rds")
```

### Predicting

```
targetData <- make_targetData(DTM)
predCountry <- make_predictions("classif.randomForest", 
	list(mtry = floor(sqrt(ncol(trainingDataMulticlass) - 1))),
	trainingDataMulticlass, targetData, MODEL_TYPE, filter = FALSE)

predCountry$response <- as.character(predCountry$response)
predCountry$response[as.character(predRelevance) == "Irrelevant"] <- "Irrelevant"
predCountry$response <- as.factor(predCountry$response)

saveRDS(predRelevance, "predRelevance.Rds")
saveRDS(predCountry, "predCountry.Rds")
```

# Consolidating results

We now consolidate all results to be used for analysis.
Because this uses the complete corpus, the following code is not executed.

## Cross-walk between databases

The following code chunks perform a cross-walk between the topic model and EndNote databases using document id: `EndNoteIdcorpus` and `EndNoteIdLDA`.
This ensures that the two databases can communicate and are aligned.


### Data loading

```
englishCorpus_file <- "F:/hguillon/research/exploitation/R/latin_america/data/english_corpus.Rds"
englishCorpus <- readRDS(englishCorpus_file)

in_corpus_file <- "in_corpus.Rds"
in_corpus <- readRDS(in_corpus_file)

predCountry <- readRDS("predCountry.Rds") # aligned with englishCorpus
predRelevance <- readRDS("predRelevance.Rds") # aligned with englishCorpus
topicDocs <- readRDS("./data/topicDocs.Rds") # aligned with englishCorpus
```
### Get document IDs

```
EndNoteIdcorpus <- get_EndNoteIdcorpus(in_corpus)
EndNoteIdLDA <- get_EndNoteIdLDA(englishCorpus)
QA_EndNoteIdCorpusLDA(EndNoteIdLDA, EndNoteIdcorpus)
```

### Align databases 

```
in_corpus <- align_dataWithEndNoteIdcorpus(in_corpus, EndNoteIdcorpus, EndNoteIdLDA)

englishCorpus <- align_dataWithEndNoteIdLDA(englishCorpus, EndNoteIdLDA, EndNoteIdcorpus)
predCountry <- align_dataWithEndNoteIdLDA(predCountry, EndNoteIdLDA, EndNoteIdcorpus)
predRelevance <- align_dataWithEndNoteIdLDA(predRelevance, EndNoteIdLDA, EndNoteIdcorpus)
topicDocs <- align_dataWithEndNoteIdLDA(topicDocs, EndNoteIdLDA, EndNoteIdcorpus)

EndNoteIdLDA <- align_dataWithEndNoteIdLDA(EndNoteIdLDA, EndNoteIdLDA, EndNoteIdcorpus)
EndNoteIdcorpus <- align_dataWithEndNoteIdcorpus(EndNoteIdcorpus, EndNoteIdcorpus, EndNoteIdLDA)

QA_EndNoteIdCorpusLDA(EndNoteIdLDA, EndNoteIdcorpus)
```

### Order them according to LDA database

```
in_corpus <- order_data(in_corpus, EndNoteIdLDA, EndNoteIdcorpus)
```

## Saving results

```
saveRDS(predRelevance, "predRelevance.Rds")
saveRDS(predCountry %>% pull(response), "predCountry.Rds")
saveRDS(predCountry %>% select(-response), "predCountryMembership.Rds")
```

```
consolidate_LDA_results(theme_type = "topic_name", save = TRUE)
consolidate_LDA_results(theme_type = "theme", save = TRUE)
consolidate_LDA_results(theme_type = "NSF_general", save = TRUE)
consolidate_LDA_results(theme_type = "NSF_specific", save = TRUE)
consolidate_LDA_results(theme_type = "theme", description = "water budget", save = TRUE)
consolidate_LDA_results(theme_type = "theme", description = "methods", save = TRUE)
consolidate_LDA_results(theme_type = "theme", description = "spatial scale", save = TRUE)
```